<div style="text-align: center; margin-bottom: 10px;">
    <font face="é»‘ä½“" size="6">å®éªŒå…­ã€Xception</font>
</div>
<table width="100%" style="margin-bottom: 4px;">
  <tr>
    <td align="left"><b>å¢è±ªè±ª</b></td>
    <td align="center"><b>202310310239</b></td>
    <td align="right"><b>æŒ‡å¯¼è€å¸ˆï¼šèƒ¡æ”¿ä¼Ÿ</b></td>
  </tr>
</table>


#### ä¸€ã€å®éªŒç›®çš„

PyTorch å¤ç°åŸè®ºæ–‡ [Xception: Deep Learning with Depthwise Separable Convolutions](https://arxiv.org/pdf/1610.02357 ) ã€‚

#### äºŒã€å®éªŒå†…å®¹
##### 1ã€Inception & Xception
Xcaption è„±èƒäº Inception ï¼ˆæ­£å¦‚å…¶åï¼ŒExtreme Inceptionï¼‰ã€‚Iception æ€æƒ³æ˜¯å°†å·ç§¯æ‹†åˆ†ä¸º cross-channel conv & Spatial convï¼Œæ›´ç²¾ç¡®æ¥è¯´å°±æ˜¯å…ˆ **1Ã—1 å·ç§¯** å°†æ•°æ®æ˜ å°„åˆ°å‡ ä¸ªä¸åŒ Channel ï¼ˆ< Input Channelï¼‰ï¼Œç„¶åå†åˆ†åˆ«ç”¨ **3Ã—3ã€5Ã—5 ** conv ï¼Œä¹Ÿå°±æ˜¯åŸPaper Figure 1æè¿°çš„è¿™æ ·ã€‚

![](photos\paper_f1.png)

è€ƒè™‘å°† incption ç®€åŒ–ï¼šå»æ‰ AVGï¼Œç„¶ååªç”¨ä¸€ç§å°ºå¯¸å·ç§¯ï¼ˆe.g. 3x3ï¼‰ï¼Œå°±å¾—åˆ°äº† Figure 2è¿™ç§ç»“æ„ã€‚

![image-20251218083046100](photos\paper_f2.png)

åœ¨ Figure 2çš„åŸºç¡€ä¸Šï¼Œç”¨ä¸€ä¸ª Channel å¾ˆå¤§çš„ **1x1** Conv å°†è¾“å…¥æ˜ å°„åˆ°ä¸€ä¸ªå¤§ Channel è¾“å‡ºä¸Šã€‚å†å°†è¿™ä¸ªè¾“å‡ºåˆ‡æˆâ€œæ®µâ€ï¼Œæ¯â€œæ®µâ€åˆ†åˆ« **3x3** Convï¼Œå°±å¾—åˆ°äº†åŸ Paper Figure 3ã€‚

![image-20251218094308530](photos\paper_f3.png)

ç„¶åæ¯ä¸ª Channel åˆ‡ä¸€â€œç‰‡â€ï¼Œå¯¹æ¯ä¸ª Channel åš 3X3 Convã€‚åŒæ—¶ä½œè€…æå‡ºäº†ä¸€ä¸ªQuestion : åˆ†å‰²çš„æ•°é‡ & å¤§å°ä¼šäº§ç”Ÿä»€ä¹ˆå½±å“ï¼Ÿå°† cross-channel convå’Œspatial convå®Œå…¨è§£è€¦åˆ†å¼€åˆç†å—ï¼Ÿå¯ä»¥è¿™æ ·åšå—ï¼Ÿ

ä½œè€…åœ¨ä¸Šè¿°å‡è®¾ä¸‹ï¼Œèµ°å‘äº†æç«¯ï¼šè¿˜æ˜¯å…ˆç”¨ 1X1 conv ï¼Œä½†æ˜¯**åˆ‡æ®µ** â¡ï¸ **åˆ‡ç‰‡** ï¼Œä¹Ÿå°±æ˜¯æ¯ä¸ª Channel åˆ‡ä¸€ç‰‡ã€‚ç„¶åæ¯ä¸ª Channel åš 3x3 Convã€‚æ­¤æ—¶è¿™ç§æç«¯ç‰ˆæœ¬å…¶å®å¾ˆåƒï¼ˆis almost identical toï¼‰depthwise separable convolution äº†ã€‚

![image-20251218085157524](photos\paper_f4.png)

ä¸ºä»€ä¹ˆâ€œå¾ˆåƒâ€ï¼Œè€Œä¸æ˜¯å°±æ˜¯ï¼Ÿ

- æ“ä½œé¡ºåºï¼šdepthwise separable convolutionï¼Œå…ˆç”¨ 3x3 conv è¿›è¡Œ spatial convï¼Œåç”¨ 1x1 conv è¿›è¡Œ cross-channel conv ï¼ˆä¹Ÿå°±æ˜¯å…ˆç©ºé—´è§£è€¦ï¼Œå†é€šé“æ··åˆã€‚ï¼‰ï¼›æç«¯ç‰ˆæœ¬ Inception å…ˆç”¨ 1x1 conv å†ç”¨3x3 conv ï¼ˆåè¿‡æ¥äº†ï¼‰ï¼›ï¼ˆ**Ps** ï¼šMobileNet V2 / Inverted Residual ï¼‰
- æ¿€æ´»å‡½æ•°å·®å¼‚ï¼šdepthwise separable convolutions é€šå¸¸å®ç°æ—¶ä¸ä¼šä½¿ç”¨éçº¿æ€§æ¿€æ´»ï¼›è€Œ Inception ä¸¤ä¸ªæ“ä½œéƒ½è·Ÿéšç€ ReLU éçº¿æ€§æ¿€æ´»ã€‚

æ“ä½œé¡ºåºè¿™é‡Œï¼Œæˆ‘å°†å…¶ç†è§£ä¸º**å…ˆå‹ç¼©å†æ‰©å±•**ï¼ˆä¸è¿‡å…¶å®åŸ Paper å®é™…å¼ºè°ƒçš„æ˜¯ å…ˆç©ºé—´è§£è€¦ï¼Œåœ¨é€šé“æ··åˆï¼‰ï¼Œä¸è¿‡åœ¨ V2 ç‰ˆæœ¬ï¼Œæ˜ç¡®äº†å°±æ˜¯è¦**å…ˆæ‰©å±•å†å‹ç¼©**ï¼Œä¸¤ä¸ªåŸå› ï¼š

- ä¿¡æ¯åœ¨â€œé«˜ç»´ç©ºé—´â€ä¸­æ›´å®¹æ˜“è¡¨è¾¾ï¼Œå…ˆå‹ç¼©åŠ¿å¿…ä¼šé€ æˆä¿¡æ¯æŸå¤±ï¼›
- Linear Bottleneck

ç‰¹åˆ«åœ°ï¼Œä½œè€…è®¤ä¸ºç¬¬ä¸€ä¸ªæ“ä½œé¡ºåºä¸Šçš„åŒºåˆ«å¹¶ä¸é‡è¦ï¼Œç‰¹åˆ«æ˜¯å› ä¸ºè¿™äº›æ“ä½œæ—¨åœ¨å †å ä½¿ç”¨ã€‚ç¬¬äºŒä¸ªæ›´åŠ é‡è¦ï¼åŸ Paper åœ¨æœ€åä¹Ÿè¿›è¡Œäº†æ¶ˆèå®éªŒæ¥è¯å®è¿™ä¸€ç‚¹ï¼ˆæ¿€æ´»å‡½æ•°å·®å¼‚ï¼‰ï¼Œç»“æœå¦‚ä¸‹ï¼š

![image-20251218133326816](photos\activation_difference.png)



#### 2ã€ç»“æ„æè¿°åŠå¤ç°

##### 2.1 æè¿° & æ€è·¯

å…³é”®å‡è®¾ï¼šCNN ç‰¹å¾å›¾ä¸­çš„è·¨é€šé“ç›¸å…³æ€§ & ç©ºé—´ç›¸å…³æ€§çš„æ˜ å°„å¯ä»¥è¢«å®Œå…¨è§£è€¦ã€‚ç„¶åä½œè€…è§£é‡Šäº†ä¸ºä»€ä¹ˆ Xception (Extreme Incption) ï¼šå› ä¸ºè¿™ä¸€å‡è®¾æ˜¯ Inception å‡è®¾çš„ pro max ç‰ˆæœ¬ğŸ˜„ã€‚é‡‡ç”¨è¿™ç§è®¾å®šï¼ŒXception ç»“æ„è¢«è§£é‡Šä¸ºåŸ Paper Figure 5 çš„æ ·å­ã€‚

![image-20251218094127615](photos/xception_architecture.png)

> The Xception architecture: the data first goes through the entry flow, then through the middle flow which is repeated eight times, and finally through the exit flow. Note that all Convolution and SeparableConvolution layers are followed by batch normalization [7] (notincluded in the diagram). All SeparableConvolution layers use a depth multiplier of 1 (no depth expansion).


Xception çš„ç‰¹å¾æå–åŸºç¡€ç”±36ä¸ª Conv layer ç»„æˆã€‚è¿™36ä¸ª Conv layer è¢«è¿›ä¸€æ­¥ç»„ç»‡æˆ14ä¸ªmoduleï¼Œé™¤äº†ç¬¬ä¸€ä¸ª & æœ€åä¸€ä¸ª module ï¼Œå…¶ä½™å‡å¸¦æœ‰ residual connection ã€‚å¯ä»¥è¯´ X ception å°±æ˜¯è¿ç»­ä½¿ç”¨ depthwise separable convolution layer å’Œ residual connection ã€‚

å®ç°æ—¶ï¼Œä¸ºäº†æ–¹ä¾¿ï¼Œä»¥ module ä¸ºå•ä½ï¼š

- Entry flowï¼š
  -  1th module æ™®é€šå·ç§¯å•ç‹¬å®ç°ï¼›
  - 2~4 moduleï¼šç»“æ„ç›¸ä¼¼ï¼ˆåŸºæœ¬éƒ½æ˜¯ä¸‹é‡‡æ ·ï¼‰ï¼Œç”¨ `_PoolENBlock` å®ç°ã€‚
- Middle flowï¼š5~ 12 module ç»“æ„ç›¸ä¼¼ï¼Œä¸”ä¸ä¸‹é‡‡æ ·ï¼Œç”¨ `_PoolMBlock` å®ç°ã€‚
- Exit Block ï¼š
  - 14th module ä¸ 2~4th module ç±»ä¼¼ï¼ˆChannel ä¸åŒï¼‰ï¼Œç”¨ `_PoolExBlock` å®ç°ã€‚



##### 2.2 å®ç°

- é¦–å…ˆå°±æ˜¯ depthwise seperabel convolution ï¼Œè°ƒç”¨ `nn.Conv2d()` åˆ†åˆ«æŒ‡å®š `groups = in_channels` ã€`kernel_size = 1` å°±èƒ½å®ç°ã€‚

```python
class SeparableConv2d(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1,
                 padding=1, dilation=1, bias=True):

        super(SeparableConv2d, self).__init__()
        self.dconv = nn.Conv2d(in_channels, in_channels, kernel_size, stride,
                               padding=padding, dilation=dilation,
                               groups=in_channels, bias=bias)
        self.pconv = nn.Conv2d(in_channels, out_channels, kernel_size=1,
                               bias=bias)
        pass

    def forward(self, x):
        return self.pconv(self.dconv(x))  # å…ˆdepthwise convï¼Œåpointwise conv
    
    pass
```

- æ®‹å·®åˆ†æ”¯ï¼ˆproject/skipï¼‰ï¼Œå°±æ˜¯ 1x1 Conv & BatchNormã€‚

```python
class ResidualConnection(nn.Sequential):
    def __init__(self, in_channels, out_channels, stride=1):
		# é»˜è®¤ä¸ä¸‹é‡‡æ ·ï¼Œåªè°ƒæ•´ Channel
        super(ResidualConnection, self).__init__(
            nn.Conv2d(in_channels, out_channels, 1, stride=stride, bias=False),
            nn.BatchNorm2d(out_channels)
        )
        pass

    pass
```

- `_PoolEnBlock` ï¼ŒæŒ‰ç…§è®ºæ–‡æ‰€è¯´ **all Convolution and SeparableConvolution layers are followed by batch normalization** ï¼Œåé¢è·Ÿ BNï¼›å¯¹äºæ¯ä¸ª moduleä¸­çš„ SeparableConv å…¶ output_channels ä¸€æ ·ï¼Œæœ€å MaxPool ä¸‹é‡‡æ ·ã€‚è¿˜æœ‰å°±æ˜¯å¢åŠ äº†ä¸€ä¸ªå‚æ•° relu1 ï¼ˆå› ä¸ºç¬¬ä¸€ä¸ª Module å‰æ²¡æœ‰æ¥ ReLu ï¼‰ã€‚å®ç°å¦‚ä¸‹ï¼š

```python
class _PoolEnBlock(nn.Module):
    def __init__(self, in_channels, out_channels, relu1=True): # é»˜è®¤æœ‰ relu
        super(_PoolEnBlock, self).__init__()
        self.project = ResidualConnection(in_channels, out_channels, stride=2)
        self.relu1 = None
        if relu1:
            self.relu1 = nn.ReLU(inplace=False) # ç‰¹åˆ«åœ°ï¼Œè¿™é‡Œè¦ä¸º Fales
            
        self.sepconv1 = SeparableConv2d(in_channels, out_channels,
                                        kernel_size=3, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)

        self.relu2 = nn.ReLU(inplace=True)
        self.sepconv2 = SeparableConv2d(out_channels, out_channels,
                                        kernel_size=3, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)

        self.maxpool = nn.MaxPool2d(3, stride=2, padding=1)
        pass

    def forward(self, x):
        identity = self.project(x)  

        if self.relu1:  # ç¬¬1ä¸ªSeparable Convå‰é¢æ²¡æœ‰ReLUï¼Œéœ€è¦åˆ¤æ–­ä¸€ä¸‹
            x = self.relu1(x)
        x = self.sepconv1(x)  # 1th
        x = self.bn1(x)

        x = self.relu2(x)
        x = self.sepconv2(x)  # 2th
        x = self.bn2(x)

        x = self.maxpool(x)  # ä¸‹é‡‡æ ·2å€
        x = x + identity  # residual connection
        return x

    pass
```

- Middle flow çš„è¯ä¸­é—´å°±æ˜¯é‡å¤ä¸‰ä¸ªç›¸åŒçš„ Block ï¼ˆChannelã€Spatial size éƒ½ä¸å˜ï¼‰ï¼Œé‡å¤å³å¯ã€‚å®ç°å¦‚ä¸‹ï¼š

```python
class _PoolMBlock(nn.Module):
    def __init__(self, in_channels=728):
        super(_PoolMBlock, self).__init__()
        
        mods = [
            nn.ReLU(inplace=False), 
            SeparableConv2d(in_channels, in_channels, 3, padding=1, bias=False),
            nn.BatchNorm2d(in_channels)
        ]
        mods *= 3  # é‡å¤ 3 æ¬¡
        self.convs = nn.Sequential(*mods)

    def forward(self, x):
        return x + self.convs(x)
```

- Exit flow ï¼Œç¬¬ä¸€ä¸ª Module ä¸¤ä¸ª Seperable Conv çš„ Out_Channels ä¸ä¸€æ ·ï¼ˆ728 & 1024ï¼‰ï¼Œç„¶åéƒ½æ˜¯ä¸‹é‡‡æ ·ä¸¤å€ã€‚å®ç°å¦‚ä¸‹ï¼š

```python
class _PoolExitBlock(nn.Module):
    def __init__(self, in_channels=728, out_channels=1024):
        super(_PoolExitBlock, self).__init__()
        self.project = ResidualConnection(in_channels, out_channels, stride=2)

        self.relu1 = nn.ReLU(inplace=False)  
        self.sepconv1 = SeparableConv2d(in_channels, in_channels,
                                        kernel_size=3, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(in_channels)

        self.relu2 = nn.ReLU(inplace=True)
        self.sepconv2 = SeparableConv2d(in_channels, out_channels,
                                        kernel_size=3, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)

        self.maxpool = nn.MaxPool2d(3, stride=2, padding=1)
        pass

    def forward(self, x):
        identity = self.project(x) 

        x = self.relu1(x)
        x = self.sepconv1(x)  # 1th
        x = self.bn1(x)

        x = self.relu2(x)
        x = self.sepconv2(x)  # 2th
        x = self.bn2(x)

        x = self.maxpool(x)  # ä¸‹é‡‡æ ·2å€

        x = x + identity  # plus
        return x

    pass
```

- æœ€åçš„è¯å°±æ˜¯ä¸»å¹²ç½‘ç»œå–½ï¼Œé™¤äº†ä¸Šè¿°å¸¦ Residual Connection çš„ Module ï¼Œå…¶å®å°±å‰©ä¸‹éœ€è¦å•ç‹¬å®ç°çš„å·ç§¯æ“ä½œäº†ï¼Œä¸åœ¨è¯¦ç»†è¯´æ˜ã€‚ç„¶ååˆå§‹åŒ–çš„è¯åŸ Paper å¹¶æœªå£°æ˜ï¼Œæˆ‘é»˜è®¤å°†å…¶ä½¿ç”¨ Kaiming Initialization ã€‚å®ç°å¦‚ä¸‹ï¼š

```python
class Xception(nn.Module):
    def __init__(self, in_channels=3, num_classes=1000):
        super(Xception, self).__init__()
        #Entry flow
        conv1 = [nn.Conv2d(in_channels, 32, 3, stride=2, padding=1, bias=False),
                 nn.BatchNorm2d(32),
                 nn.ReLU(inplace=True),]
        self.entry_conv1 = nn.Sequential(*conv1)

        conv2 = [nn.Conv2d(32, 64, 3, padding=1, bias=False),
                 nn.BatchNorm2d(64),
                 nn.ReLU(inplace=True),]
        self.entry_conv2 = nn.Sequential(*conv2)

        self.entry_block1 = _PoolEntryBlock(64, 128, relu1=False)
        self.entry_block2 = _PoolEntryBlock(128, 256)
        self.entry_block3 = _PoolEntryBlock(256, 728)

        #Middle flow
        self.middle_flow = nn.ModuleList([_PoolMiddleBlock(728) for _ in range(8)])

        #Exit flow
        self.exit_block = _PoolExitBlock(728, 1024)

        conv1 = [SeparableConv2d(1024, 1536, 3, padding=1, bias=False),
                 nn.BatchNorm2d(1536),
                 nn.ReLU(inplace=True),]
        self.exit_conv1 = nn.Sequential(*conv1)

        conv2 = [SeparableConv2d(1536, 2048, 3, padding=1, bias=False),
                 nn.BatchNorm2d(2048),
                 nn.ReLU(inplace=True),]
        self.exit_conv2 = nn.Sequential(*conv2)

        self.avgpool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Linear(2048, num_classes) 


    def _init_weights(self): # Kaiming Initialization
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, 0, 0.01)
                nn.init.constant_(m.bias, 0)

    def forward(self, x):
        # Entry
        x = self.entry_conv1(x)
        x = self.entry_conv2(x)

        x = self.entry_block1(x)
        x = self.entry_block2(x)
        x = self.entry_block3(x)

        # Middle
        for block in self.middle_flow:
            x = block(x)

        # Exit
        x = self.exit_block(x)
        x = self.exit_conv1(x)
        x = self.exit_conv2(x)

        # FCnet
        x = self.avgpool(x)
        x = x.view(x.shape[0], -1)
        x = self.fc(x)

        return x
```

#### 3ã€æ¨¡å‹æ£€éªŒ

å› ä¸ºåŸ Paper æ˜¯åœ¨ ImageNet-1k & JFT ç­‰æ•°æ®é›†ä¸Šè¿›è¡Œæµ‹è¯•çš„ï¼Œé«˜åˆ†è¾¨ç‡å›¾åƒã€‚å› æ­¤æ¨¡å‹æ£€éªŒè¿™é‡Œæˆ‘é€‰æ‹©è¿›è¡Œè¿‡æ‹Ÿåˆå®éªŒæ£€éªŒæ­£ç¡®æ€§ã€‚æ‰€é‡‡å–çš„æ•°æ®ä¸ºä¸ªäºº Google Drive ä¸Šå­˜æ”¾çš„ ImageNet-1k éªŒè¯é›†æ•°æ®ï¼ŒæŠ½å–100å¼ å›¾ç‰‡æ„æˆ MiDaSe ï¼Œåœ¨å…¶ä¸Šè®­ç»ƒ 50 Epoches ä»¥è®©å…¶è¾¾åˆ°å¿«é€Ÿè¿‡æ‹Ÿåˆçš„ç›®çš„ã€‚å…·ä½“å®ç°ä»£ç è¾ƒé•¿è¿™é‡Œä¸å†è¿›è¡Œå±•ç¤ºï¼Œè¯¦è§ `overfitting.ipynb` ã€‚è¯¦æƒ…ç»˜åˆ¶å¦‚ä¸‹ï¼š

![kaiming](photos/\kaiming.png)

- å¯çŸ¥ Loss è¿…é€Ÿä¸‹é™ï¼Œå‡†ç¡®ç‡å¿«é€Ÿä¸Šå‡ï¼Œè¶³ä»¥è¯´æ˜æ¨¡å‹å®ç°æ­£ç¡®æ€§ï¼

#### ä¸‰ã€å®éªŒæ€»ç»“

1ã€ä¸¥æ ¼æŒ‰ç…§è®ºæ–‡å¤ç° Xception ï¼Œæ¯ä¸ª module çš„ç¬¬ä¸€ä¸ªæ“ä½œæ˜¯ ReLu (1th Module é™¤å¤–)ã€‚åœ¨å¯¹åº”å¾ˆå¤šå«æ®‹å·®è¿æ¥çš„éƒ¨åˆ†ï¼Œæ­¤æ—¶éœ€è¦ä¿ç•™åŸå§‹å€¼è¿›è¡Œ `+ x` çš„æ“ä½œã€‚å› ä¸ºè¾“å…¥åˆ†è¾¨ç‡æ¯”è¾ƒå¤§ï¼Œæˆ‘ä¸ºäº†èŠ‚çœå†…å­˜ï¼Œåœ¨åˆæ¬¡å®ç°æ—¶åœ¨ `self.relu1 = nn.ReLU(inplace=False) ` è¿™é‡Œç”¨äº† **inplace=True** åœ¨åŸåœ°ï¼ˆ+=ï¼‰å¼ é‡ä¿®æ”¹ï¼Œè¿™å°±å¯¼è‡´å…¶ç›´æ¥ä¿®æ”¹è¾“å…¥ Tensor çš„å†…å­˜æ•°æ®ã€‚è€Œåƒæ®‹å·®è¿æ¥è¿™ç§æƒ…å†µæ¢¯åº¦è®¡ç®—éœ€è¦ç”¨åˆ°åŸå§‹ Tensor çš„æ—¶å€™å°±å‡ºç° **Error** ï¼š

```py
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation:
```

![err_architecture](photos/err_architecture.png)

2ã€åé¢æˆ‘å°†å…¶æ”¹ä¸º `inplace = False` å°±è¡Œäº†ã€‚ä¸è¿‡é€šè¿‡ğŸ”å‘ç°è¿˜æœ‰ä¿®æ”¹æ–¹å¼ï¼šå°±æ˜¯è½¬åŒ–ä¸ºæ›´æ˜¾æ˜¾å¼çš„ out = out + identityï¼Œä¹Ÿå°±æ˜¯å°† out += identity è¿™ç§åŸåœ°ä¿®æ”¹è½¬åŒ–ä¸º out = out + identity è¿™ç§éåŸåœ°ä¿®æ”¹å°±è¡Œäº†ã€‚

3ã€å…¶ä»–éƒ¨åˆ†è¿™é‡Œä¸å†è¿›è¡Œé˜è¿°ï¼Œå®éªŒå†…å®¹éƒ¨åˆ†å·²æœ‰è¾ƒè¯¦ç»†è¯´æ˜



